{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toronto Metropolitan University $~~~~~~~~~~~~~~~~~~~~~~~~~~~$ $~~~~~~~~~~~~~~~~~$ Laboratory of Innovations in Transportation (LiTans)\n",
    "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ \n",
    "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$\n",
    "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ \n",
    "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ \n",
    "\n",
    "# <center> <font color='royalblue'> “Impacts of the COVID-19 Pandemic on Ridesourcing Services in Small Towns and Large Cities”</font> </center> \n",
    "\n",
    "# <center> <font color='royalblue'> Part Three: Direct Demand Model </font> </center> \n",
    "\n",
    "\n",
    "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$ \n",
    "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$\n",
    "\n",
    "# <center>Nael Alsaleh & Bilal Farooq </center>\n",
    "$~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$\n",
    "# August 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "\n",
    "1. Modelling dataset that includes all the variables shown below.\n",
    "\n",
    "Med_Income_Orig, Pop_den_Orig, Per_Male_Orig, Working_age_Orig, Elderly_Orig, Avg_Hhold_Orig, Married_Orig, HighSchool_higher_Orig, Bachelor_Higher_Orig, \n",
    "Commute_PC_Orig, Com_LU_Orig, Gov_LU_Orig, Ind_LU_Orig, Parks_LU_Orig, Res_LU_Orig, Med_Income_Des, Pop_den_Des, Per_Male_Des, Per_Male_Des, Working_age_Des, Elderly_Des, Avg_Hhold_Des, Married_Des, HighSchool_higher_Des, Bachelor_Higher_Des, Commute_PC_Des, Com_LU_Des, Gov_LU_Des, Ind_LU_Des, Parks_LU_Des, Res_LU_Des, Avgerage_Temp, Precipitation, Snow', Cases, Deaths, Hospitalizations, Positive_Rate, Partially_Vaccinated, Fully_Vaccinated, Stringency_Index, Year, Month, Day_of_week, Weekend, and trips.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "In order to run this code successfully, you will need to:\n",
    "1. Latest version of Python\n",
    "2. Installing the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "The implementation of the direct demand model has been divided into two parts:\n",
    "\n",
    "1. Modelling\n",
    "2. Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='forestgreen'> Part 1. Modelling </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A) Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B) Import the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['Med_Income_Orig', 'Pop_den_Orig', 'Per_Male_Orig','Working_age_Orig', 'Elderly_Orig', 'Avg_Hhold_Orig', 'Married_Orig','HighSchool_higher_Orig', 'Bachelor_Higher_Orig', \n",
    "'Commute_PC_Orig', 'Com_LU_Orig', 'Gov_LU_Orig', 'Ind_LU_Orig', 'Parks_LU_Orig', 'Res_LU_Orig', 'Med_Income_Des', 'Pop_den_Des', 'Per_Male_Des', 'Per_Male_Des', 'Working_age_Des', 'Elderly_Des', 'Avg_Hhold_Des', \n",
    "'Married_Des', 'HighSchool_higher_Des', 'Bachelor_Higher_Des', 'Commute_PC_Des', 'Com_LU_Des', 'Gov_LU_Des', 'Ind_LU_Des', 'Parks_LU_Des', 'Res_LU_Des', 'Avgerage_Temp', 'Precipitation', 'Snow', 'Cases', 'Deaths',                 \n",
    "'Hospitalizations', 'Positive_Rate', 'Partially_Vaccinated', 'Fully_Vaccinated', 'Stringency_Index', 'Year', 'Month', 'Day_of_week', 'Weekend', 'Trips']\n",
    "\n",
    "# Define the type of the variables\n",
    "dtype={\n",
    "        'Med_Income_Orig': float,\n",
    "        'Pop_den_Orig':float,\n",
    "        'Per_Male_Orig':float,\n",
    "        'Working_age_Orig':float,\n",
    "        'Elderly_Orig':float,\n",
    "        'Avg_Hhold_Orig':float,\n",
    "        'Married_Orig':float,\n",
    "        'HighSchool_higher_Orig':float,\n",
    "        'Bachelor_Higher_Orig':float,\n",
    "        'Commute_PC_Orig':float,\n",
    "        'Com_LU_Orig':float,\n",
    "        'Gov_LU_Orig':float,\n",
    "        'Ind_LU_Orig':float,\n",
    "        'Parks_LU_Orig':float,\n",
    "        'Res_LU_Orig':float,\n",
    "        'Med_Income_Des':float,\n",
    "        'Pop_den_Des':float,\n",
    "        'Per_Male_Des':float,\n",
    "        'Working_age_Des':float,\n",
    "        'Elderly_Des':float,\n",
    "        'Avg_Hhold_Des':float,\n",
    "        'Married_Des':float,\n",
    "        'HighSchool_higher_Des':float,\n",
    "        'Bachelor_Higher_Des':float,\n",
    "        'Commute_PC_Des':float,\n",
    "        'Com_LU_Des':float,\n",
    "        'Gov_LU_Des':float,\n",
    "        'Ind_LU_Des':float,\n",
    "        'Parks_LU_Des':float,\n",
    "        'Res_LU_Des':float, \n",
    "        'Avgerage_Temp': float,\n",
    "        'Snow': float,\n",
    "        'Precipitation': float,\n",
    "        'Cases': int,\n",
    "        'Deaths': int,\n",
    "        'Hospitalizations': int,\n",
    "        'Positive_Rate': float,\n",
    "        'Partially_Vaccinated': float,\n",
    "        'Fully_Vaccinated': float,\n",
    "        'Stringency_Index': float,\n",
    "        'Year':float,\n",
    "        'Month': float,\n",
    "        'Day_of_week': float,\n",
    "        'Weekend': float,\n",
    "        'Trips': float\n",
    "      }\n",
    "\n",
    "Chunck = pd.read_csv('DirectDemandSample.csv',chunksize=3000000,usecols= columns, dtype = dtype)\n",
    "Modelling_Data = pd.concat(Chunck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Modelling_Data[['Med_Income_Orig', 'Pop_den_Orig', 'Per_Male_Orig','Working_age_Orig', 'Elderly_Orig', 'Avg_Hhold_Orig', 'Married_Orig', 'Bachelor_Higher_Orig', \n",
    "'Commute_PC_Orig', 'Com_LU_Orig', 'Gov_LU_Orig', 'Ind_LU_Orig', 'Parks_LU_Orig', 'Res_LU_Orig', 'Med_Income_Des',  'Pop_den_Des', 'Per_Male_Des', 'Working_age_Des', 'Elderly_Des', 'Avg_Hhold_Des', \n",
    "'Married_Des', 'Bachelor_Higher_Des', 'Commute_PC_Des', 'Com_LU_Des', 'Gov_LU_Des', 'Ind_LU_Des', 'Parks_LU_Des', 'Res_LU_Des', 'Avgerage_Temp', 'Precipitation', 'Snow', 'Cases', 'Deaths',                 \n",
    "'Hospitalizations', 'Positive_Rate', 'Partially_Vaccinated', 'Fully_Vaccinated', 'Stringency_Index', 'Year', 'Month', 'Day_of_week', 'Weekend']]\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "Y = Modelling_Data['Trips']\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size=0.2, random_state = 1)\n",
    "\n",
    "# Print the shape of training and testing datasets\n",
    "print (X_Train.shape)\n",
    "print (Y_Train.shape)\n",
    "print (X_Test.shape)\n",
    "print (Y_Test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C) Find the optimum Hyperparameters for the Trip Distribution Model (Random Forest Model) using Bayesian Optimization Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the space\n",
    "space = {'criterion': hp.choice('criterion', ['squared_error', 'absolute_error']),\n",
    "        'max_depth': hp.quniform('max_depth', 10, 150, 10),\n",
    "        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2']),\n",
    "        'min_samples_leaf': hp.uniform ('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.choice('n_estimators', [10, 50, 100, 150, 300]),\n",
    "        'max_leaf_nodes' : hp.choice('max_leaf_nodes', [10, 50, 100, None]),\n",
    "        'bootstrap' : hp.choice('bootstrap', [True, False])\n",
    "         }\n",
    "#Define the Objective function\n",
    "def objective(space):\n",
    "    model = RandomForestRegressor(criterion = space['criterion'], \n",
    "                                  max_depth = space['max_depth'],\n",
    "                                  max_features = space['max_features'],\n",
    "                                  min_samples_leaf = space['min_samples_leaf'],\n",
    "                                  min_samples_split = space['min_samples_split'],\n",
    "                                  n_estimators = space['n_estimators'],\n",
    "                                  max_leaf_nodes =space['max_leaf_nodes'],\n",
    "                                  bootstrap = space['bootstrap'],\n",
    "                                 )\n",
    "    \n",
    "\n",
    "    scores = cross_val_score(model, X_Train, Y_Train, cv = 5, scoring='neg_mean_squared_error').mean()  # Note that CV should be changed if you are running \n",
    "    #return scores.mean()\n",
    "    return {'loss': - scores, 'status': STATUS_OK }\n",
    "    \n",
    "trials = Trials()\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = 65,\n",
    "            trials= trials)\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D)  Apply the optimal  architectures  of  the  algorithms  on  the  testing  set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = {0: 'squared_error', 1: 'absolute_error'}\n",
    "feat = {0: 'auto', 1: 'sqrt', 2: 'log2'}\n",
    "est = {0: 10, 1: 50, 2: 100, 3: 150, 4: 300}\n",
    "leaf_node = {0: 10, 1: 50, 2: 100, 3: None}\n",
    "boot = {0: True, 1: False}\n",
    "\n",
    "\n",
    "trainedforest = RandomForestRegressor( criterion = crit[best['criterion']], \n",
    "                                       max_depth = best['max_depth'], \n",
    "                                       max_features = feat[best['max_features']], \n",
    "                                       min_samples_leaf = best['min_samples_leaf'], \n",
    "                                       min_samples_split = best['min_samples_split'], \n",
    "                                       n_estimators = est[best['n_estimators']], \n",
    "                                       max_leaf_nodes = leaf_node[best['max_leaf_nodes']],\n",
    "                                       bootstrap = boot[best['bootstrap']],\n",
    "                                      ).fit(X_Train,Y_Train)\n",
    "\n",
    "predictionforest = trainedforest.predict(X_Test)\n",
    "\n",
    "y_true = Y_Test \n",
    "y_pred = predictionforest \n",
    "\n",
    "# Obtain the main performance metrics\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_true, y_pred))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_true, y_pred))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_true, y_pred)))\n",
    "print('R^2:', metrics.r2_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='forestgreen'> Part 2: Model Interpretation Using SHAP (SHapley  Additive  exPlanations)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Test_new = pd.DataFrame(X_Test, columns = ['Med_Income_Orig', 'Pop_den_Orig', 'Per_Male_Orig','Working_age_Orig', 'Elderly_Orig', 'Avg_Hhold_Orig', 'Married_Orig', 'Bachelor_Higher_Orig', \n",
    "'Commute_PC_Orig', 'Com_LU_Orig', 'Gov_LU_Orig', 'Ind_LU_Orig', 'Parks_LU_Orig', 'Res_LU_Orig', 'Med_Income_Des',  'Pop_den_Des', 'Per_Male_Des', 'Working_age_Des', 'Elderly_Des', 'Avg_Hhold_Des', \n",
    "'Married_Des', 'Bachelor_Higher_Des', 'Commute_PC_Des', 'Com_LU_Des', 'Gov_LU_Des', 'Ind_LU_Des', 'Parks_LU_Des', 'Res_LU_Des', 'Avgerage_Temp', 'Precipitation', 'Snow', 'Cases', 'Deaths',                 \n",
    "'Hospitalizations', 'Positive_Rate', 'Partially_Vaccinated', 'Fully_Vaccinated', 'Stringency_Index', 'Year', 'Month', 'Day_of_week', 'Weekend'])\n",
    "\n",
    "import shap\n",
    "X_train_summary = shap.kmeans(X_Train, 4)\n",
    "\n",
    "# using the kmeans summary\n",
    "rf_explainer_ = shap.KernelExplainer(trainedforest.predict,X_train_summary)\n",
    "\n",
    "rf_shap_values_ = rf_explainer_.shap_values(X_Test_new.iloc[0:1000])\n",
    "\n",
    "shap.summary_plot(rf_shap_values_, X_Test_new.iloc[0:1000], show=False, max_display=X.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e64fb616c2a40ddd74d2e651fffdf997e434dc29da653839def51e45d993589a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
